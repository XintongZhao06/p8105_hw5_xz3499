---
title: "p8105_mtp_xz3499"
author: "Xintong Zhao"
date: "`r Sys.Date()`"
output: github_document
---

```{r, echo=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,           
  warning = FALSE,       
  message = FALSE           
)
```

```{r}
library(tidyverse)
library(ggplot2)
library(broom)
library(knitr)
```

## Problem 1
```{r}
check_duplicate_birthdays <- function(n) {
  birthdays <- sample(1:365, size = n, replace = TRUE)
  repeated_bday = length(unique(birthdays)) < n
  repeated_bday
}

set.seed(123)

birthday_sim_results <- expand_grid(
  group_size = 2:50,
  iteration = 1:10000
) %>%
  mutate(
    has_duplicate = map_lgl(group_size, check_duplicate_birthdays)
  ) %>%
  group_by(group_size) %>%
  summarize(
    prob_duplicate = mean(has_duplicate)
  )

birthday_plot <- birthday_sim_results %>%
  ggplot(aes(x = group_size, y = prob_duplicate)) +
  geom_line(color = "steelblue", size = 1) +
  geom_point(color = "steelblue", size = 1) +
  labs(
    title = "Probability of Shared Birthday by Group Size",
    x = "Number of People in Group",
    y = "Probability of at Least One Shared Birthday",
    caption = "Based on 10,000 simulations per group size"
  ) +
  theme_minimal() +
  scale_y_continuous(labels = scales::percent_format()) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    panel.grid.minor = element_blank()
  )

birthday_plot
```

Comment:

The probability curve grows rapidly in an S shape. When the number of people in a group is 23, the probability exceeds 50%. When the number of people in a group is around 40, the probability exceeds 90%. This goes against our intuition that 183 people are needed to have a 50% chance. This is because as the number of people increases, the possible number of birthday pairings grows exponentially, causing the probability of repetition to increase rapidly.

## Problem 2

```{r}
n <- 30
sigma <- 5
alpha <- 0.05
mu_values <- c(0, 1, 2, 3, 4, 5, 6)
n_sims <- 5000

# Function to simulate data and perform t-test
simulate_ttest <- function(mu, n, sigma) {
  data <- rnorm(n, mean = mu, sd = sigma)
  test_result <- t.test(data, mu = 0)
  tidy_result <- broom::tidy(test_result)
  
  return(tibble(
    mu_hat = tidy_result$estimate,
    p_value = tidy_result$p.value
  ))
}

set.seed(123)

power_sim_results <- expand_grid(
  true_mu = mu_values,
  iter = 1:n_sims
) %>%
  mutate(
    test_results = map(true_mu, ~ simulate_ttest(.x, n, sigma))
  ) %>%
  unnest(test_results) %>%
  mutate(
    rejected = p_value < alpha
  )
```

### The power of the test

```{r}
power_curve <- power_sim_results %>%
  group_by(true_mu) %>%
  summarize(
    power = mean(rejected),
    se_power = sqrt(power * (1 - power) / n())
  ) %>%
  ggplot(aes(x = true_mu, y = power)) +
  geom_line(color = "steelblue", size = 1.2) +
  geom_point(color = "steelblue", size = 2) +
  geom_errorbar(aes(ymin = power - 1.96 * se_power, 
                    ymax = power + 1.96 * se_power), 
                width = 0.1, color = "steelblue") +
  labs(
    title = "Statistical Power vs True Mean",
    subtitle = paste("n =", n, ", σ =", sigma, ", α =", alpha),
    x = "True Mean μ",
    y = "Power (Probability of Rejecting H₀)"
  ) +
  scale_y_continuous(limits = c(0, 1), labels = scales::percent_format()) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5)
  )

print(power_curve)
```

Description:

The statistical Power increases rapidly with the increase of the True Mean (μ). The efficacy exceeds 50% approximately when μ = 2, exceeds 80% when μ = 3, and almost reaches 100% when μ ≥ 4. This is because the larger the effect size, the easier it is for the sample mean to deviate from the expected value (μ₀) under the null hypothesis, and thus it is more likely to be detected by statistical tests. 

### Mean estimation comparison

```{r}
mean_estimates <- power_sim_results %>%
  group_by(true_mu) %>%
  summarize(
    overall_mean = mean(mu_hat),
    rejected_mean = mean(mu_hat[rejected])
  )

estimate_plot <- ggplot(mean_estimates, aes(x = true_mu)) +
  geom_line(aes(y = overall_mean, color = "All Samples"), size = 1.2) +
  geom_point(aes(y = overall_mean, color = "All Samples"), size = 2) +
  geom_line(aes(y = rejected_mean, color = "Rejected Samples"), size = 1.2) +
  geom_point(aes(y = rejected_mean, color = "Rejected Samples"), size = 2) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", 
              color = "gray", alpha = 0.7) +
  labs(
    title = "Mean Estimates vs True Mean",
    subtitle = paste("n =", n, ", σ =", sigma),
    x = "True Mean μ",
    y = "Estimated Mean μ̂",
    color = "Sample Type"
  ) +
  scale_color_manual(values = c("All Samples" = "lightblue", 
                               "Rejected Samples" = "lightpink")) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5),
    legend.position = "bottom"
  )

print(estimate_plot)
```

Description:

When μ is small (0 to 2), the number of rejected samples is higher than that of all samples and above the dotted line of the true mean, which means that the samples that reject the null hypothesis tend to have a higher estimated mean. When μ is large (≥3), the two lines almost coincide, and the estimated value is close to the true μ. This is because when the effect size is small, the null hypothesis will only be rejected when the sample mean is unexpectedly high. Therefore, the average estimate of the rejected sample will be systematically high (selective bias). When the effect size is large enough, almost all samples can reject the null hypothesis. At this point, the rejected samples are almost consistent with the population sample, and the bias disappears.

## Problem 3

```{r}
homicide_data <- read_csv("data/homicide-data.csv")

summary(homicide_data)
```

Description:

The original dataset of this homicide case contains `nrow(homicide_data)` records, spanning from January 1, 2007 to November 5, 2015. The data includes case identification information, detailed information of the victim (including name, race, age and gender), geographic information (city, state and longitude and latitude coordinates of occurrence), as well as the handling status of the case. Among them, there are 60 missing values in the longitude and latitude coordinates, the age of the victim is stored in character form, and there is an abnormal entry of 9 digits in the maximum report date. In addition, the data integrity of other variables is relatively good, and it completely records the details of homicide cases in multiple cities in the United States over the past nine years.

```{r}
homicide_summary <- homicide_data %>%
  mutate(
    city_state = paste(city, state, sep = ", ")
  ) %>%
  group_by(city_state) %>%
  summarize(
    total_homicides = n(),
    unsolved_homicides = sum(disposition %in% c("Closed without arrest", "Open/No arrest"))
  ) %>%
  filter(total_homicides > 0) 
```

```{r}
baltimore_data <- homicide_summary %>%
  filter(city_state == "Baltimore, MD")

baltimore_test <- prop.test(
  x = baltimore_data$unsolved_homicides,
  n = baltimore_data$total_homicides
)

baltimore_results <- broom::tidy(baltimore_test) %>%
  select(estimate, conf.low, conf.high) %>%
  mutate(city_state = "Baltimore, MD")

kable(baltimore_results)
```

```{r}
all_city_tests <- homicide_summary %>%
  mutate(
    test_results = map2(unsolved_homicides, total_homicides,
                       ~broom::tidy(prop.test(x = .x, n = .y)))
  ) %>%
  unnest(test_results) %>%
  select(city_state, total_homicides, unsolved_homicides, 
         estimate, conf.low, conf.high)

kable(all_city_tests)
```



```{r}
homicide_plot <- all_city_tests %>%
  mutate(city_state = fct_reorder(city_state, estimate)) %>%
  ggplot(aes(x = estimate, y = city_state)) +
  geom_point(color = "#A23B72", size = 2, alpha = 0.8) +
  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high), 
                 height = 0.3, color = "#A23B72", alpha = 0.6, size = 0.8) +
  geom_vline(xintercept = mean(all_city_tests$estimate), 
             linetype = "dashed", color = "#34495E", alpha = 0.7) +
  labs(
    title = "Proportion of Unsolved Homicides by City",
    subtitle = "Cities ordered by proportion of unsolved cases | Dashed line = overall average",
    x = "Proportion Unsolved",
    y = NULL,
    caption = "Data: Washington Post Homicide Database\nError bars show 95% confidence intervals"
  ) +
  theme_minimal(base_size = 11) +
  theme(
    plot.title = element_text(face = "bold", hjust = 0.5, color = "#2C3E50"),
    plot.subtitle = element_text(hjust = 0.5, color = "#7F8C8D", size = 10),
    plot.caption = element_text(color = "#95A5A6", size = 9),
    axis.text.y = element_text(size = 5, color = "#2C3E50"),
    axis.text.x = element_text(color = "#2C3E50"),
    axis.title.x = element_text(color = "#2C3E50", face = "bold"),
    panel.grid.major.y = element_line(color = "#ECF0F1", size = 0.3),
    panel.grid.minor = element_blank(),
    plot.background = element_rect(fill = "white", color = NA)
  ) +
  scale_x_continuous(labels = scales::percent_format(accuracy = 1))

# Display the plot
print(homicide_plot)
```

